{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "586a97dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import data\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8f032c65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
       "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
       "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
       "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
       "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
       "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
       "\n",
       "        V26       V27       V28  Amount  Class  \n",
       "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4  0.502292  0.219422  0.215153   69.99      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read csv\n",
    "df = pd.read_csv('creditcard_cc_fruad.csv')\n",
    "df.head()   #seee first few column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3de5901a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset has 284807 rows and 31 columns.\n"
     ]
    }
   ],
   "source": [
    "print(f\"The dataset has {df.shape[0]} rows and {df.shape[1]} columns.\") #see how many data in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c3f25ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    }
   ],
   "source": [
    "#split feature and Class\n",
    "\n",
    "x = df.drop('Class', axis=1)\n",
    "y = df['Class']\n",
    "\n",
    "shape = x.shape \n",
    "num_of_features = shape[1]\n",
    "print(num_of_features) #see how many x feature in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dd28f69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and testing datasets by train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=123, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c0786831",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert dataframe to tensor\n",
    "x_train_tensor = torch.from_numpy(x_train.to_numpy()).float()\n",
    "y_train_tensor = torch.from_numpy(y_train.to_numpy()).long() #to long as using CrossEntropyLoss, label need to be long\n",
    "x_test_tensor = torch.from_numpy(x_test.to_numpy()).float()\n",
    "y_test_tensor = torch.from_numpy(y_test.to_numpy()).long()\n",
    "\n",
    "#tensor to tensor dataset\n",
    "train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(x_test_tensor, y_test_tensor)\n",
    "\n",
    "#tensor dataset to dataloader\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=64)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "82fc1273",
   "metadata": {},
   "outputs": [],
   "source": [
    "#decoder for classify two class from features output two logic\n",
    "#30 features ->64 ->relu ->48 ->relu ->32 ->relu ->2 , in interface, argmax at last\n",
    "\n",
    "class CcFruadClassifier(nn.Module):\n",
    "    def __init__(self, num_of_features):\n",
    "        super(CcFruadClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(num_of_features, 64)\n",
    "        self.fc2 = nn.Linear(64, 48)\n",
    "        self.fc3 = nn.Linear(48, 32)\n",
    "        self.fc4 = nn.Linear(32, 2)  # Output layer for two-class classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3f3bbf61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if can use cuda, no cud use cpu instead\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = CcFruadClassifier(num_of_features).to(device) # to gpu orcpu\n",
    "loss_formula = torch.nn.CrossEntropyLoss() #use cross entropy loss for binary classification can also be use for multi-class\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b4cddf86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200], Train Loss: 1.7480\n",
      "Epoch [1/200], Test Loss: 0.0653\n",
      "Epoch [2/200], Train Loss: 0.0967\n",
      "Epoch [2/200], Test Loss: 0.0413\n",
      "Epoch [3/200], Train Loss: 0.0834\n",
      "Epoch [3/200], Test Loss: 0.2721\n",
      "Epoch [4/200], Train Loss: 0.0604\n",
      "Epoch [4/200], Test Loss: 0.5715\n",
      "Epoch [5/200], Train Loss: 0.1139\n",
      "Epoch [5/200], Test Loss: 0.3613\n",
      "Epoch [6/200], Train Loss: 0.0650\n",
      "Epoch [6/200], Test Loss: 0.1691\n",
      "Epoch [7/200], Train Loss: 0.0422\n",
      "Epoch [7/200], Test Loss: 0.1159\n",
      "Epoch [8/200], Train Loss: 0.0313\n",
      "Epoch [8/200], Test Loss: 0.0299\n",
      "Epoch [9/200], Train Loss: 0.0416\n",
      "Epoch [9/200], Test Loss: 0.0168\n",
      "Epoch [10/200], Train Loss: 0.0283\n",
      "Epoch [10/200], Test Loss: 0.0199\n",
      "Epoch [11/200], Train Loss: 0.0210\n",
      "Epoch [11/200], Test Loss: 0.0154\n",
      "Epoch [12/200], Train Loss: 0.0176\n",
      "Epoch [12/200], Test Loss: 0.0126\n",
      "Epoch [13/200], Train Loss: 0.0185\n",
      "Epoch [13/200], Test Loss: 0.0128\n",
      "Epoch [14/200], Train Loss: 0.0138\n",
      "Epoch [14/200], Test Loss: 0.0144\n",
      "Epoch [15/200], Train Loss: 0.0127\n",
      "Epoch [15/200], Test Loss: 0.0146\n",
      "Epoch [16/200], Train Loss: 0.0117\n",
      "Epoch [16/200], Test Loss: 0.0134\n",
      "Epoch [17/200], Train Loss: 0.0137\n",
      "Epoch [17/200], Test Loss: 0.0146\n",
      "Epoch [18/200], Train Loss: 0.0136\n",
      "Epoch [18/200], Test Loss: 0.0143\n",
      "Epoch [19/200], Train Loss: 0.0103\n",
      "Epoch [19/200], Test Loss: 0.0123\n",
      "Epoch [20/200], Train Loss: 0.0123\n",
      "Epoch [20/200], Test Loss: 0.0119\n",
      "Epoch [21/200], Train Loss: 0.0101\n",
      "Epoch [21/200], Test Loss: 0.0118\n",
      "Epoch [22/200], Train Loss: 0.0091\n",
      "Epoch [22/200], Test Loss: 0.0118\n",
      "Epoch [23/200], Train Loss: 0.0206\n",
      "Epoch [23/200], Test Loss: 0.0128\n",
      "Epoch [24/200], Train Loss: 0.0129\n",
      "Epoch [24/200], Test Loss: 0.0120\n",
      "Epoch [25/200], Train Loss: 0.0097\n",
      "Epoch [25/200], Test Loss: 0.0117\n",
      "Epoch [26/200], Train Loss: 0.0116\n",
      "Epoch [26/200], Test Loss: 0.0108\n",
      "Epoch [27/200], Train Loss: 0.0129\n",
      "Epoch [27/200], Test Loss: 0.0122\n",
      "Epoch [28/200], Train Loss: 0.0133\n",
      "Epoch [28/200], Test Loss: 0.0129\n",
      "Epoch [29/200], Train Loss: 0.0078\n",
      "Epoch [29/200], Test Loss: 0.0160\n",
      "Epoch [30/200], Train Loss: 0.0096\n",
      "Epoch [30/200], Test Loss: 0.0121\n",
      "Epoch [31/200], Train Loss: 0.0092\n",
      "Epoch [31/200], Test Loss: 0.0131\n",
      "Epoch [32/200], Train Loss: 0.0076\n",
      "Epoch [32/200], Test Loss: 0.0110\n",
      "Epoch [33/200], Train Loss: 0.0104\n",
      "Epoch [33/200], Test Loss: 0.0159\n",
      "Epoch [34/200], Train Loss: 0.0083\n",
      "Epoch [34/200], Test Loss: 0.0247\n",
      "Epoch [35/200], Train Loss: 0.0083\n",
      "Epoch [35/200], Test Loss: 0.0125\n",
      "Epoch [36/200], Train Loss: 0.0084\n",
      "Epoch [36/200], Test Loss: 0.0129\n",
      "Epoch [37/200], Train Loss: 0.0063\n",
      "Epoch [37/200], Test Loss: 0.0095\n",
      "Epoch [38/200], Train Loss: 0.0078\n",
      "Epoch [38/200], Test Loss: 0.0232\n",
      "Epoch [39/200], Train Loss: 0.0062\n",
      "Epoch [39/200], Test Loss: 0.0185\n",
      "Epoch [40/200], Train Loss: 0.0114\n",
      "Epoch [40/200], Test Loss: 0.0116\n",
      "Epoch [41/200], Train Loss: 0.0085\n",
      "Epoch [41/200], Test Loss: 0.0091\n",
      "Epoch [42/200], Train Loss: 0.0064\n",
      "Epoch [42/200], Test Loss: 0.0157\n",
      "Epoch [43/200], Train Loss: 0.0059\n",
      "Epoch [43/200], Test Loss: 0.0121\n",
      "Epoch [44/200], Train Loss: 0.0060\n",
      "Epoch [44/200], Test Loss: 0.0099\n",
      "Epoch [45/200], Train Loss: 0.0075\n",
      "Epoch [45/200], Test Loss: 0.0093\n",
      "Epoch [46/200], Train Loss: 0.0100\n",
      "Epoch [46/200], Test Loss: 0.0143\n",
      "Epoch [47/200], Train Loss: 0.0072\n",
      "Epoch [47/200], Test Loss: 0.0091\n",
      "Epoch [48/200], Train Loss: 0.0076\n",
      "Epoch [48/200], Test Loss: 0.0321\n",
      "Epoch [49/200], Train Loss: 0.0075\n",
      "Epoch [49/200], Test Loss: 0.0117\n",
      "Epoch [50/200], Train Loss: 0.0063\n",
      "Epoch [50/200], Test Loss: 0.0071\n",
      "Epoch [51/200], Train Loss: 0.0066\n",
      "Epoch [51/200], Test Loss: 0.0106\n",
      "Epoch [52/200], Train Loss: 0.0145\n",
      "Epoch [52/200], Test Loss: 0.0111\n",
      "Epoch [53/200], Train Loss: 0.0063\n",
      "Epoch [53/200], Test Loss: 0.0173\n",
      "Epoch [54/200], Train Loss: 0.0058\n",
      "Epoch [54/200], Test Loss: 0.0080\n",
      "Epoch [55/200], Train Loss: 0.0056\n",
      "Epoch [55/200], Test Loss: 0.0077\n",
      "Epoch [56/200], Train Loss: 0.0055\n",
      "Epoch [56/200], Test Loss: 0.0106\n",
      "Epoch [57/200], Train Loss: 0.0053\n",
      "Epoch [57/200], Test Loss: 0.0077\n",
      "Epoch [58/200], Train Loss: 0.0053\n",
      "Epoch [58/200], Test Loss: 0.0059\n",
      "Epoch [59/200], Train Loss: 0.0055\n",
      "Epoch [59/200], Test Loss: 0.0058\n",
      "Epoch [60/200], Train Loss: 0.0057\n",
      "Epoch [60/200], Test Loss: 0.0069\n",
      "Epoch [61/200], Train Loss: 0.0055\n",
      "Epoch [61/200], Test Loss: 0.0095\n",
      "Epoch [62/200], Train Loss: 0.0050\n",
      "Epoch [62/200], Test Loss: 0.0063\n",
      "Epoch [63/200], Train Loss: 0.0054\n",
      "Epoch [63/200], Test Loss: 0.0062\n",
      "Epoch [64/200], Train Loss: 0.0052\n",
      "Epoch [64/200], Test Loss: 0.0058\n",
      "Epoch [65/200], Train Loss: 0.0047\n",
      "Epoch [65/200], Test Loss: 0.0072\n",
      "Epoch [66/200], Train Loss: 0.0050\n",
      "Epoch [66/200], Test Loss: 0.0069\n",
      "Epoch [67/200], Train Loss: 0.0047\n",
      "Epoch [67/200], Test Loss: 0.0053\n",
      "Epoch [68/200], Train Loss: 0.0056\n",
      "Epoch [68/200], Test Loss: 0.0104\n",
      "Epoch [69/200], Train Loss: 0.0063\n",
      "Epoch [69/200], Test Loss: 0.0059\n",
      "Epoch [70/200], Train Loss: 0.0076\n",
      "Epoch [70/200], Test Loss: 0.0148\n",
      "Epoch [71/200], Train Loss: 0.0057\n",
      "Epoch [71/200], Test Loss: 0.0061\n",
      "Epoch [72/200], Train Loss: 0.0048\n",
      "Epoch [72/200], Test Loss: 0.0063\n",
      "Epoch [73/200], Train Loss: 0.0053\n",
      "Epoch [73/200], Test Loss: 0.0061\n",
      "Epoch [74/200], Train Loss: 0.0047\n",
      "Epoch [74/200], Test Loss: 0.0065\n",
      "Epoch [75/200], Train Loss: 0.0047\n",
      "Epoch [75/200], Test Loss: 0.0062\n",
      "Epoch [76/200], Train Loss: 0.0049\n",
      "Epoch [76/200], Test Loss: 0.0066\n",
      "Epoch [77/200], Train Loss: 0.0046\n",
      "Epoch [77/200], Test Loss: 0.0073\n",
      "Epoch [78/200], Train Loss: 0.0047\n",
      "Epoch [78/200], Test Loss: 0.0056\n",
      "Epoch [79/200], Train Loss: 0.0045\n",
      "Epoch [79/200], Test Loss: 0.0054\n",
      "Epoch [80/200], Train Loss: 0.0046\n",
      "Epoch [80/200], Test Loss: 0.0066\n",
      "Epoch [81/200], Train Loss: 0.0046\n",
      "Epoch [81/200], Test Loss: 0.0056\n",
      "Epoch [82/200], Train Loss: 0.0046\n",
      "Epoch [82/200], Test Loss: 0.0059\n",
      "Epoch [83/200], Train Loss: 0.0044\n",
      "Epoch [83/200], Test Loss: 0.0055\n",
      "Epoch [84/200], Train Loss: 0.0052\n",
      "Epoch [84/200], Test Loss: 0.0054\n",
      "Epoch [85/200], Train Loss: 0.0044\n",
      "Epoch [85/200], Test Loss: 0.0056\n",
      "Epoch [86/200], Train Loss: 0.0044\n",
      "Epoch [86/200], Test Loss: 0.0057\n",
      "Epoch [87/200], Train Loss: 0.0053\n",
      "Epoch [87/200], Test Loss: 0.0059\n",
      "Epoch [88/200], Train Loss: 0.0045\n",
      "Epoch [88/200], Test Loss: 0.0071\n",
      "Epoch [89/200], Train Loss: 0.0049\n",
      "Epoch [89/200], Test Loss: 0.0059\n",
      "Epoch [90/200], Train Loss: 0.0045\n",
      "Epoch [90/200], Test Loss: 0.0055\n",
      "Epoch [91/200], Train Loss: 0.0044\n",
      "Epoch [91/200], Test Loss: 0.0061\n",
      "Epoch [92/200], Train Loss: 0.0047\n",
      "Epoch [92/200], Test Loss: 0.0077\n",
      "Epoch [93/200], Train Loss: 0.0051\n",
      "Epoch [93/200], Test Loss: 0.0062\n",
      "Epoch [94/200], Train Loss: 0.0041\n",
      "Epoch [94/200], Test Loss: 0.0054\n",
      "Epoch [95/200], Train Loss: 0.0047\n",
      "Epoch [95/200], Test Loss: 0.0060\n",
      "Epoch [96/200], Train Loss: 0.0046\n",
      "Epoch [96/200], Test Loss: 0.0068\n",
      "Epoch [97/200], Train Loss: 0.0042\n",
      "Epoch [97/200], Test Loss: 0.0050\n",
      "Epoch [98/200], Train Loss: 0.0043\n",
      "Epoch [98/200], Test Loss: 0.0067\n",
      "Epoch [99/200], Train Loss: 0.0045\n",
      "Epoch [99/200], Test Loss: 0.0060\n",
      "Epoch [100/200], Train Loss: 0.0042\n",
      "Epoch [100/200], Test Loss: 0.0056\n",
      "Epoch [101/200], Train Loss: 0.0045\n",
      "Epoch [101/200], Test Loss: 0.0056\n",
      "Epoch [102/200], Train Loss: 0.0043\n",
      "Epoch [102/200], Test Loss: 0.0050\n",
      "Epoch [103/200], Train Loss: 0.0042\n",
      "Epoch [103/200], Test Loss: 0.0055\n",
      "Epoch [104/200], Train Loss: 0.0043\n",
      "Epoch [104/200], Test Loss: 0.0049\n",
      "Epoch [105/200], Train Loss: 0.0042\n",
      "Epoch [105/200], Test Loss: 0.0061\n",
      "Epoch [106/200], Train Loss: 0.0041\n",
      "Epoch [106/200], Test Loss: 0.0060\n",
      "Epoch [107/200], Train Loss: 0.0046\n",
      "Epoch [107/200], Test Loss: 0.0060\n",
      "Epoch [108/200], Train Loss: 0.0039\n",
      "Epoch [108/200], Test Loss: 0.0056\n",
      "Epoch [109/200], Train Loss: 0.0041\n",
      "Epoch [109/200], Test Loss: 0.0050\n",
      "Epoch [110/200], Train Loss: 0.0040\n",
      "Epoch [110/200], Test Loss: 0.0050\n",
      "Epoch [111/200], Train Loss: 0.0043\n",
      "Epoch [111/200], Test Loss: 0.0058\n",
      "Epoch [112/200], Train Loss: 0.0044\n",
      "Epoch [112/200], Test Loss: 0.0054\n",
      "Epoch [113/200], Train Loss: 0.0042\n",
      "Epoch [113/200], Test Loss: 0.0076\n",
      "Epoch [114/200], Train Loss: 0.0040\n",
      "Epoch [114/200], Test Loss: 0.0059\n",
      "Epoch [115/200], Train Loss: 0.0041\n",
      "Epoch [115/200], Test Loss: 0.0091\n",
      "Epoch [116/200], Train Loss: 0.0044\n",
      "Epoch [116/200], Test Loss: 0.0053\n",
      "Epoch [117/200], Train Loss: 0.0039\n",
      "Epoch [117/200], Test Loss: 0.0058\n",
      "Epoch [118/200], Train Loss: 0.0041\n",
      "Epoch [118/200], Test Loss: 0.0091\n",
      "Epoch [119/200], Train Loss: 0.0044\n",
      "Epoch [119/200], Test Loss: 0.0057\n",
      "Epoch [120/200], Train Loss: 0.0039\n",
      "Epoch [120/200], Test Loss: 0.0051\n",
      "Epoch [121/200], Train Loss: 0.0039\n",
      "Epoch [121/200], Test Loss: 0.0057\n",
      "Epoch [122/200], Train Loss: 0.0053\n",
      "Epoch [122/200], Test Loss: 0.0079\n",
      "Epoch [123/200], Train Loss: 0.0038\n",
      "Epoch [123/200], Test Loss: 0.0052\n",
      "Epoch [124/200], Train Loss: 0.0040\n",
      "Epoch [124/200], Test Loss: 0.0058\n",
      "Epoch [125/200], Train Loss: 0.0043\n",
      "Epoch [125/200], Test Loss: 0.0049\n",
      "Epoch [126/200], Train Loss: 0.0041\n",
      "Epoch [126/200], Test Loss: 0.0050\n",
      "Epoch [127/200], Train Loss: 0.0039\n",
      "Epoch [127/200], Test Loss: 0.0059\n",
      "Epoch [128/200], Train Loss: 0.0038\n",
      "Epoch [128/200], Test Loss: 0.0058\n",
      "Epoch [129/200], Train Loss: 0.0040\n",
      "Epoch [129/200], Test Loss: 0.0062\n",
      "Epoch [130/200], Train Loss: 0.0079\n",
      "Epoch [130/200], Test Loss: 0.0060\n",
      "Epoch [131/200], Train Loss: 0.0037\n",
      "Epoch [131/200], Test Loss: 0.0054\n",
      "Epoch [132/200], Train Loss: 0.0038\n",
      "Epoch [132/200], Test Loss: 0.0049\n",
      "Epoch [133/200], Train Loss: 0.0039\n",
      "Epoch [133/200], Test Loss: 0.0052\n",
      "Epoch [134/200], Train Loss: 0.0040\n",
      "Epoch [134/200], Test Loss: 0.0048\n",
      "Epoch [135/200], Train Loss: 0.0039\n",
      "Epoch [135/200], Test Loss: 0.0062\n",
      "Epoch [136/200], Train Loss: 0.0039\n",
      "Epoch [136/200], Test Loss: 0.0062\n",
      "Epoch [137/200], Train Loss: 0.0041\n",
      "Epoch [137/200], Test Loss: 0.0093\n",
      "Epoch [138/200], Train Loss: 0.0042\n",
      "Epoch [138/200], Test Loss: 0.0057\n",
      "Epoch [139/200], Train Loss: 0.0038\n",
      "Epoch [139/200], Test Loss: 0.0080\n",
      "Epoch [140/200], Train Loss: 0.0039\n",
      "Epoch [140/200], Test Loss: 0.0059\n",
      "Epoch [141/200], Train Loss: 0.0041\n",
      "Epoch [141/200], Test Loss: 0.0052\n",
      "Epoch [142/200], Train Loss: 0.0041\n",
      "Epoch [142/200], Test Loss: 0.0051\n",
      "Epoch [143/200], Train Loss: 0.0039\n",
      "Epoch [143/200], Test Loss: 0.0069\n",
      "Epoch [144/200], Train Loss: 0.0038\n",
      "Epoch [144/200], Test Loss: 0.0055\n",
      "Epoch [145/200], Train Loss: 0.0042\n",
      "Epoch [145/200], Test Loss: 0.0054\n",
      "Epoch [146/200], Train Loss: 0.0048\n",
      "Epoch [146/200], Test Loss: 0.0228\n",
      "Epoch [147/200], Train Loss: 0.0049\n",
      "Epoch [147/200], Test Loss: 0.0058\n",
      "Epoch [148/200], Train Loss: 0.0040\n",
      "Epoch [148/200], Test Loss: 0.0055\n",
      "Epoch [149/200], Train Loss: 0.0041\n",
      "Epoch [149/200], Test Loss: 0.0059\n",
      "Epoch [150/200], Train Loss: 0.0040\n",
      "Epoch [150/200], Test Loss: 0.0068\n",
      "Epoch [151/200], Train Loss: 0.0039\n",
      "Epoch [151/200], Test Loss: 0.0063\n",
      "Epoch [152/200], Train Loss: 0.0041\n",
      "Epoch [152/200], Test Loss: 0.0065\n",
      "Epoch [153/200], Train Loss: 0.0043\n",
      "Epoch [153/200], Test Loss: 0.0052\n",
      "Epoch [154/200], Train Loss: 0.0038\n",
      "Epoch [154/200], Test Loss: 0.0057\n",
      "Epoch [155/200], Train Loss: 0.0038\n",
      "Epoch [155/200], Test Loss: 0.0057\n",
      "Epoch [156/200], Train Loss: 0.0045\n",
      "Epoch [156/200], Test Loss: 0.0073\n",
      "Epoch [157/200], Train Loss: 0.0055\n",
      "Epoch [157/200], Test Loss: 0.0057\n",
      "Epoch [158/200], Train Loss: 0.0085\n",
      "Epoch [158/200], Test Loss: 0.0091\n",
      "Epoch [159/200], Train Loss: 0.0058\n",
      "Epoch [159/200], Test Loss: 0.0067\n",
      "Epoch [160/200], Train Loss: 0.0061\n",
      "Epoch [160/200], Test Loss: 0.0063\n",
      "Epoch [161/200], Train Loss: 0.0048\n",
      "Epoch [161/200], Test Loss: 0.0056\n",
      "Epoch [162/200], Train Loss: 0.0039\n",
      "Epoch [162/200], Test Loss: 0.0053\n",
      "Epoch [163/200], Train Loss: 0.0041\n",
      "Epoch [163/200], Test Loss: 0.0051\n",
      "Epoch [164/200], Train Loss: 0.0039\n",
      "Epoch [164/200], Test Loss: 0.0058\n",
      "Epoch [165/200], Train Loss: 0.0046\n",
      "Epoch [165/200], Test Loss: 0.0064\n",
      "Epoch [166/200], Train Loss: 0.0039\n",
      "Epoch [166/200], Test Loss: 0.0059\n",
      "Epoch [167/200], Train Loss: 0.0041\n",
      "Epoch [167/200], Test Loss: 0.0064\n",
      "Epoch [168/200], Train Loss: 0.0038\n",
      "Epoch [168/200], Test Loss: 0.0052\n",
      "Epoch [169/200], Train Loss: 0.0049\n",
      "Epoch [169/200], Test Loss: 0.0047\n",
      "Epoch [170/200], Train Loss: 0.0037\n",
      "Epoch [170/200], Test Loss: 0.0055\n",
      "Epoch [171/200], Train Loss: 0.0038\n",
      "Epoch [171/200], Test Loss: 0.0051\n",
      "Epoch [172/200], Train Loss: 0.0038\n",
      "Epoch [172/200], Test Loss: 0.0054\n",
      "Epoch [173/200], Train Loss: 0.0040\n",
      "Epoch [173/200], Test Loss: 0.0051\n",
      "Epoch [174/200], Train Loss: 0.0045\n",
      "Epoch [174/200], Test Loss: 0.0084\n",
      "Epoch [175/200], Train Loss: 0.0042\n",
      "Epoch [175/200], Test Loss: 0.0057\n",
      "Epoch [176/200], Train Loss: 0.0040\n",
      "Epoch [176/200], Test Loss: 0.0053\n",
      "Epoch [177/200], Train Loss: 0.0042\n",
      "Epoch [177/200], Test Loss: 0.0064\n",
      "Epoch [178/200], Train Loss: 0.0058\n",
      "Epoch [178/200], Test Loss: 0.0071\n",
      "Epoch [179/200], Train Loss: 0.0038\n",
      "Epoch [179/200], Test Loss: 0.0056\n",
      "Epoch [180/200], Train Loss: 0.0040\n",
      "Epoch [180/200], Test Loss: 0.0058\n",
      "Epoch [181/200], Train Loss: 0.0039\n",
      "Epoch [181/200], Test Loss: 0.0059\n",
      "Epoch [182/200], Train Loss: 0.0037\n",
      "Epoch [182/200], Test Loss: 0.0063\n",
      "Epoch [183/200], Train Loss: 0.0043\n",
      "Epoch [183/200], Test Loss: 0.0055\n",
      "Epoch [184/200], Train Loss: 0.0041\n",
      "Epoch [184/200], Test Loss: 0.0067\n",
      "Epoch [185/200], Train Loss: 0.0038\n",
      "Epoch [185/200], Test Loss: 0.0059\n",
      "Epoch [186/200], Train Loss: 0.0049\n",
      "Epoch [186/200], Test Loss: 0.0053\n",
      "Epoch [187/200], Train Loss: 0.0036\n",
      "Epoch [187/200], Test Loss: 0.0050\n",
      "Epoch [188/200], Train Loss: 0.0040\n",
      "Epoch [188/200], Test Loss: 0.0052\n",
      "Epoch [189/200], Train Loss: 0.0039\n",
      "Epoch [189/200], Test Loss: 0.0054\n",
      "Epoch [190/200], Train Loss: 0.0043\n",
      "Epoch [190/200], Test Loss: 0.0069\n",
      "Epoch [191/200], Train Loss: 0.0038\n",
      "Epoch [191/200], Test Loss: 0.0049\n",
      "Epoch [192/200], Train Loss: 0.0037\n",
      "Epoch [192/200], Test Loss: 0.0052\n",
      "Epoch [193/200], Train Loss: 0.0040\n",
      "Epoch [193/200], Test Loss: 0.0056\n",
      "Epoch [194/200], Train Loss: 0.0038\n",
      "Epoch [194/200], Test Loss: 0.0052\n",
      "Epoch [195/200], Train Loss: 0.0037\n",
      "Epoch [195/200], Test Loss: 0.0059\n",
      "Epoch [196/200], Train Loss: 0.0036\n",
      "Epoch [196/200], Test Loss: 0.0056\n",
      "Epoch [197/200], Train Loss: 0.0038\n",
      "Epoch [197/200], Test Loss: 0.0173\n",
      "Epoch [198/200], Train Loss: 0.0039\n",
      "Epoch [198/200], Test Loss: 0.0054\n",
      "Epoch [199/200], Train Loss: 0.0040\n",
      "Epoch [199/200], Test Loss: 0.0063\n",
      "Epoch [200/200], Train Loss: 0.0040\n",
      "Epoch [200/200], Test Loss: 0.0051\n"
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "\n",
    "#start training model\n",
    "for epoch in range(epochs):  # Number of epochs\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_dataloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_formula(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Train Loss: {running_loss/len(train_dataloader):.4f}\")\n",
    "\n",
    "    # Evaluate on test set\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            test_loss += loss_formula(outputs, labels).item()\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Test Loss: {test_loss/len(test_dataloader):.4f}\")\n",
    "\n",
    "    if epoch %20 == 0:\n",
    "        torch.save(model.state_dict(), \"checkpoint{}.pt\".format(epoch+1))\n",
    "torch.save(model.state_dict(), \"checkpoint{}.pt\".format(epoch+1))   #save last epoch result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
