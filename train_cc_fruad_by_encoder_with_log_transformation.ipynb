{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "586a97dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import data\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8f032c65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
       "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
       "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
       "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
       "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
       "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
       "\n",
       "        V26       V27       V28  Amount  Class  \n",
       "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4  0.502292  0.219422  0.215153   69.99      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read csv\n",
    "df = pd.read_csv('creditcard_cc_fruad.csv')\n",
    "df.head()   #seee first few column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3de5901a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset has 284807 rows and 31 columns.\n"
     ]
    }
   ],
   "source": [
    "print(f\"The dataset has {df.shape[0]} rows and {df.shape[1]} columns.\") #see how many data in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "95923340",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\3563463\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount', 'Class']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\3563463\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\arraylike.py:399: RuntimeWarning: divide by zero encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "#log transformation to see if performance improve\n",
    "# Create a copy of the DataFrame to avoid changing the original\n",
    "df_transformed = df.copy()\n",
    "\n",
    "#this part code from https://www.kaggle.com/code/annastasy/anomaly-detection-credit-card-fraud\n",
    "# Function to handle log transformation for skewed data\n",
    "\n",
    "# Feature names\n",
    "features = df.columns[:-1]\n",
    "def log_transform_skewed(column):\n",
    "    # For positive and zero values (log1p avoids log(0) errors)\n",
    "    transformed = np.where(column >= 0, np.log1p(column), -np.log1p(-column))\n",
    "    return transformed\n",
    "\n",
    "# Compute skewness before transformation\n",
    "skewness_before = df.skew()\n",
    "\n",
    "# Apply transformation to skewed columns\n",
    "for col in features:\n",
    "    if abs(df[col].skew()) > 0.75:  # Threshold for skewness\n",
    "        df_transformed[col] = log_transform_skewed(df[col])\n",
    "\n",
    "# Compute skewness after transformation\n",
    "skewness_after = df_transformed.skew()\n",
    "\n",
    "# Compare skewness before and after\n",
    "skewness_comparison = pd.DataFrame({\n",
    "    'Skewness Before': skewness_before,\n",
    "    'Skewness After': skewness_after\n",
    "})\n",
    "\n",
    "# Print the comparison\n",
    "skewness_comparison\n",
    "\n",
    "print(df_transformed.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c3f25ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    }
   ],
   "source": [
    "#split feature and Class\n",
    "\n",
    "x = df_transformed.drop('Class', axis=1)\n",
    "y = df_transformed['Class']\n",
    "\n",
    "shape = x.shape \n",
    "num_of_features = shape[1]\n",
    "print(num_of_features) #see how many x feature in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dd28f69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and testing datasets by train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=123, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c0786831",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert dataframe to tensor\n",
    "x_train_tensor = torch.from_numpy(x_train.to_numpy()).float()\n",
    "y_train_tensor = torch.from_numpy(y_train.to_numpy()).long() #to long as using CrossEntropyLoss, label need to be long\n",
    "x_test_tensor = torch.from_numpy(x_test.to_numpy()).float()\n",
    "y_test_tensor = torch.from_numpy(y_test.to_numpy()).long()\n",
    "\n",
    "#tensor to tensor dataset\n",
    "train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(x_test_tensor, y_test_tensor)\n",
    "\n",
    "#tensor dataset to dataloader\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=64)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "82fc1273",
   "metadata": {},
   "outputs": [],
   "source": [
    "#decoder for classify two class from features output two logic\n",
    "#30 features ->64 ->relu ->48 ->relu ->32 ->relu ->2 , in interface, argmax at last\n",
    "\n",
    "class CcFruadClassifier(nn.Module):\n",
    "    def __init__(self, num_of_features):\n",
    "        super(CcFruadClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(num_of_features, 64)\n",
    "        self.fc2 = nn.Linear(64, 48)\n",
    "        self.fc3 = nn.Linear(48, 32)\n",
    "        self.fc4 = nn.Linear(32, 2)  # Output layer for two-class classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3f3bbf61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if can use cuda, no cud use cpu instead\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = CcFruadClassifier(num_of_features).to(device) # to gpu orcpu\n",
    "loss_formula = torch.nn.CrossEntropyLoss() #use cross entropy loss for binary classification can also be use for multi-class\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b4cddf86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200], Train Loss: 3.9325\n",
      "Epoch [1/200], Test Loss: 0.1352\n",
      "Epoch [2/200], Train Loss: 0.2782\n",
      "Epoch [2/200], Test Loss: 0.0272\n",
      "Epoch [3/200], Train Loss: 0.3685\n",
      "Epoch [3/200], Test Loss: 0.2887\n",
      "Epoch [4/200], Train Loss: 0.1828\n",
      "Epoch [4/200], Test Loss: 0.0217\n",
      "Epoch [5/200], Train Loss: 0.2209\n",
      "Epoch [5/200], Test Loss: 0.2384\n",
      "Epoch [6/200], Train Loss: 0.1869\n",
      "Epoch [6/200], Test Loss: 0.7921\n",
      "Epoch [7/200], Train Loss: 0.1697\n",
      "Epoch [7/200], Test Loss: 0.0242\n",
      "Epoch [8/200], Train Loss: 0.0748\n",
      "Epoch [8/200], Test Loss: 0.0199\n",
      "Epoch [9/200], Train Loss: 0.0962\n",
      "Epoch [9/200], Test Loss: 0.0213\n",
      "Epoch [10/200], Train Loss: 0.0645\n",
      "Epoch [10/200], Test Loss: 0.0228\n",
      "Epoch [11/200], Train Loss: 0.1639\n",
      "Epoch [11/200], Test Loss: 0.0496\n",
      "Epoch [12/200], Train Loss: 0.0757\n",
      "Epoch [12/200], Test Loss: 0.1329\n",
      "Epoch [13/200], Train Loss: 0.0611\n",
      "Epoch [13/200], Test Loss: 0.3028\n",
      "Epoch [14/200], Train Loss: 0.0459\n",
      "Epoch [14/200], Test Loss: 0.0322\n",
      "Epoch [15/200], Train Loss: 0.0693\n",
      "Epoch [15/200], Test Loss: 0.0164\n",
      "Epoch [16/200], Train Loss: 0.0395\n",
      "Epoch [16/200], Test Loss: 0.0278\n",
      "Epoch [17/200], Train Loss: 0.0322\n",
      "Epoch [17/200], Test Loss: 0.0204\n",
      "Epoch [18/200], Train Loss: 0.0250\n",
      "Epoch [18/200], Test Loss: 0.0182\n",
      "Epoch [19/200], Train Loss: 0.0273\n",
      "Epoch [19/200], Test Loss: 0.0849\n",
      "Epoch [20/200], Train Loss: 0.0307\n",
      "Epoch [20/200], Test Loss: 0.3399\n",
      "Epoch [21/200], Train Loss: 0.0428\n",
      "Epoch [21/200], Test Loss: 0.5144\n",
      "Epoch [22/200], Train Loss: 0.0380\n",
      "Epoch [22/200], Test Loss: 0.0080\n",
      "Epoch [23/200], Train Loss: 0.0445\n",
      "Epoch [23/200], Test Loss: 0.0153\n",
      "Epoch [24/200], Train Loss: 0.0497\n",
      "Epoch [24/200], Test Loss: 0.0155\n",
      "Epoch [25/200], Train Loss: 0.0692\n",
      "Epoch [25/200], Test Loss: 0.2443\n",
      "Epoch [26/200], Train Loss: 0.0587\n",
      "Epoch [26/200], Test Loss: 0.0176\n",
      "Epoch [27/200], Train Loss: 0.0309\n",
      "Epoch [27/200], Test Loss: 0.0144\n",
      "Epoch [28/200], Train Loss: 0.0271\n",
      "Epoch [28/200], Test Loss: 0.0098\n",
      "Epoch [29/200], Train Loss: 0.0299\n",
      "Epoch [29/200], Test Loss: 0.0460\n",
      "Epoch [30/200], Train Loss: 0.0183\n",
      "Epoch [30/200], Test Loss: 0.0186\n",
      "Epoch [31/200], Train Loss: 0.0343\n",
      "Epoch [31/200], Test Loss: 0.1846\n",
      "Epoch [32/200], Train Loss: 0.0598\n",
      "Epoch [32/200], Test Loss: 0.0143\n",
      "Epoch [33/200], Train Loss: 0.0359\n",
      "Epoch [33/200], Test Loss: 0.0664\n",
      "Epoch [34/200], Train Loss: 0.0492\n",
      "Epoch [34/200], Test Loss: 0.0256\n",
      "Epoch [35/200], Train Loss: 0.0335\n",
      "Epoch [35/200], Test Loss: 0.0676\n",
      "Epoch [36/200], Train Loss: 0.0252\n",
      "Epoch [36/200], Test Loss: 0.0174\n",
      "Epoch [37/200], Train Loss: 0.0535\n",
      "Epoch [37/200], Test Loss: 0.0129\n",
      "Epoch [38/200], Train Loss: 0.0465\n",
      "Epoch [38/200], Test Loss: 0.0464\n",
      "Epoch [39/200], Train Loss: 0.0222\n",
      "Epoch [39/200], Test Loss: 0.0139\n",
      "Epoch [40/200], Train Loss: 0.0254\n",
      "Epoch [40/200], Test Loss: 0.0159\n",
      "Epoch [41/200], Train Loss: 0.0275\n",
      "Epoch [41/200], Test Loss: 0.0143\n",
      "Epoch [42/200], Train Loss: 0.0175\n",
      "Epoch [42/200], Test Loss: 0.0116\n",
      "Epoch [43/200], Train Loss: 0.0107\n",
      "Epoch [43/200], Test Loss: 0.0787\n",
      "Epoch [44/200], Train Loss: 0.0145\n",
      "Epoch [44/200], Test Loss: 0.0159\n",
      "Epoch [45/200], Train Loss: 0.0226\n",
      "Epoch [45/200], Test Loss: 0.1383\n",
      "Epoch [46/200], Train Loss: 0.0582\n",
      "Epoch [46/200], Test Loss: 0.0174\n",
      "Epoch [47/200], Train Loss: 0.0155\n",
      "Epoch [47/200], Test Loss: 0.0126\n",
      "Epoch [48/200], Train Loss: 0.0245\n",
      "Epoch [48/200], Test Loss: 0.1875\n",
      "Epoch [49/200], Train Loss: 0.0274\n",
      "Epoch [49/200], Test Loss: 0.0116\n",
      "Epoch [50/200], Train Loss: 0.0146\n",
      "Epoch [50/200], Test Loss: 0.0128\n",
      "Epoch [51/200], Train Loss: 0.0288\n",
      "Epoch [51/200], Test Loss: 0.0116\n",
      "Epoch [52/200], Train Loss: 0.0344\n",
      "Epoch [52/200], Test Loss: 0.0129\n",
      "Epoch [53/200], Train Loss: 0.0255\n",
      "Epoch [53/200], Test Loss: 0.0135\n",
      "Epoch [54/200], Train Loss: 0.0187\n",
      "Epoch [54/200], Test Loss: 0.0316\n",
      "Epoch [55/200], Train Loss: 0.0236\n",
      "Epoch [55/200], Test Loss: 0.0101\n",
      "Epoch [56/200], Train Loss: 0.0160\n",
      "Epoch [56/200], Test Loss: 0.0106\n",
      "Epoch [57/200], Train Loss: 0.0427\n",
      "Epoch [57/200], Test Loss: 0.0173\n",
      "Epoch [58/200], Train Loss: 0.0148\n",
      "Epoch [58/200], Test Loss: 0.0258\n",
      "Epoch [59/200], Train Loss: 0.0183\n",
      "Epoch [59/200], Test Loss: 0.0216\n",
      "Epoch [60/200], Train Loss: 0.0132\n",
      "Epoch [60/200], Test Loss: 0.0426\n",
      "Epoch [61/200], Train Loss: 0.0405\n",
      "Epoch [61/200], Test Loss: 0.0519\n",
      "Epoch [62/200], Train Loss: 0.0241\n",
      "Epoch [62/200], Test Loss: 0.1280\n",
      "Epoch [63/200], Train Loss: 0.0144\n",
      "Epoch [63/200], Test Loss: 0.0662\n",
      "Epoch [64/200], Train Loss: 0.0193\n",
      "Epoch [64/200], Test Loss: 0.0122\n",
      "Epoch [65/200], Train Loss: 0.0093\n",
      "Epoch [65/200], Test Loss: 0.0161\n",
      "Epoch [66/200], Train Loss: 0.0085\n",
      "Epoch [66/200], Test Loss: 0.0177\n",
      "Epoch [67/200], Train Loss: 0.0237\n",
      "Epoch [67/200], Test Loss: 0.0279\n",
      "Epoch [68/200], Train Loss: 0.0176\n",
      "Epoch [68/200], Test Loss: 0.0104\n",
      "Epoch [69/200], Train Loss: 0.0140\n",
      "Epoch [69/200], Test Loss: 0.0151\n",
      "Epoch [70/200], Train Loss: 0.0082\n",
      "Epoch [70/200], Test Loss: 0.0132\n",
      "Epoch [71/200], Train Loss: 0.0161\n",
      "Epoch [71/200], Test Loss: 0.0180\n",
      "Epoch [72/200], Train Loss: 0.0128\n",
      "Epoch [72/200], Test Loss: 0.0120\n",
      "Epoch [73/200], Train Loss: 0.0129\n",
      "Epoch [73/200], Test Loss: 0.0130\n",
      "Epoch [74/200], Train Loss: 0.0098\n",
      "Epoch [74/200], Test Loss: 0.0745\n",
      "Epoch [75/200], Train Loss: 0.0084\n",
      "Epoch [75/200], Test Loss: 0.0117\n",
      "Epoch [76/200], Train Loss: 0.0102\n",
      "Epoch [76/200], Test Loss: 0.0138\n",
      "Epoch [77/200], Train Loss: 0.0088\n",
      "Epoch [77/200], Test Loss: 0.0152\n",
      "Epoch [78/200], Train Loss: 0.0091\n",
      "Epoch [78/200], Test Loss: 0.0919\n",
      "Epoch [79/200], Train Loss: 0.0105\n",
      "Epoch [79/200], Test Loss: 0.0124\n",
      "Epoch [80/200], Train Loss: 0.0113\n",
      "Epoch [80/200], Test Loss: 0.0113\n",
      "Epoch [81/200], Train Loss: 0.0073\n",
      "Epoch [81/200], Test Loss: 0.0132\n",
      "Epoch [82/200], Train Loss: 0.0099\n",
      "Epoch [82/200], Test Loss: 0.0126\n",
      "Epoch [83/200], Train Loss: 0.0072\n",
      "Epoch [83/200], Test Loss: 0.0123\n",
      "Epoch [84/200], Train Loss: 0.0066\n",
      "Epoch [84/200], Test Loss: 0.0125\n",
      "Epoch [85/200], Train Loss: 0.0078\n",
      "Epoch [85/200], Test Loss: 0.0121\n",
      "Epoch [86/200], Train Loss: 0.0064\n",
      "Epoch [86/200], Test Loss: 0.0127\n",
      "Epoch [87/200], Train Loss: 0.0081\n",
      "Epoch [87/200], Test Loss: 0.0128\n",
      "Epoch [88/200], Train Loss: 0.0154\n",
      "Epoch [88/200], Test Loss: 0.0585\n",
      "Epoch [89/200], Train Loss: 0.0082\n",
      "Epoch [89/200], Test Loss: 0.0246\n",
      "Epoch [90/200], Train Loss: 0.0062\n",
      "Epoch [90/200], Test Loss: 0.0117\n",
      "Epoch [91/200], Train Loss: 0.0068\n",
      "Epoch [91/200], Test Loss: 0.0117\n",
      "Epoch [92/200], Train Loss: 0.0057\n",
      "Epoch [92/200], Test Loss: 0.0063\n",
      "Epoch [93/200], Train Loss: 0.0074\n",
      "Epoch [93/200], Test Loss: 0.0135\n",
      "Epoch [94/200], Train Loss: 0.0063\n",
      "Epoch [94/200], Test Loss: 0.0057\n",
      "Epoch [95/200], Train Loss: 0.0059\n",
      "Epoch [95/200], Test Loss: 0.0071\n",
      "Epoch [96/200], Train Loss: 0.0096\n",
      "Epoch [96/200], Test Loss: 0.0056\n",
      "Epoch [97/200], Train Loss: 0.0058\n",
      "Epoch [97/200], Test Loss: 0.0065\n",
      "Epoch [98/200], Train Loss: 0.0061\n",
      "Epoch [98/200], Test Loss: 0.0055\n",
      "Epoch [99/200], Train Loss: 0.0068\n",
      "Epoch [99/200], Test Loss: 0.0100\n",
      "Epoch [100/200], Train Loss: 0.0061\n",
      "Epoch [100/200], Test Loss: 0.0088\n",
      "Epoch [101/200], Train Loss: 0.0076\n",
      "Epoch [101/200], Test Loss: 0.0101\n",
      "Epoch [102/200], Train Loss: 0.0053\n",
      "Epoch [102/200], Test Loss: 0.0103\n",
      "Epoch [103/200], Train Loss: 0.0057\n",
      "Epoch [103/200], Test Loss: 0.0095\n",
      "Epoch [104/200], Train Loss: 0.0076\n",
      "Epoch [104/200], Test Loss: 0.0073\n",
      "Epoch [105/200], Train Loss: 0.0054\n",
      "Epoch [105/200], Test Loss: 0.0106\n",
      "Epoch [106/200], Train Loss: 0.0058\n",
      "Epoch [106/200], Test Loss: 0.0064\n",
      "Epoch [107/200], Train Loss: 0.0131\n",
      "Epoch [107/200], Test Loss: 0.0712\n",
      "Epoch [108/200], Train Loss: 0.0071\n",
      "Epoch [108/200], Test Loss: 0.0441\n",
      "Epoch [109/200], Train Loss: 0.0090\n",
      "Epoch [109/200], Test Loss: 0.0340\n",
      "Epoch [110/200], Train Loss: 0.0066\n",
      "Epoch [110/200], Test Loss: 0.0097\n",
      "Epoch [111/200], Train Loss: 0.0054\n",
      "Epoch [111/200], Test Loss: 0.0082\n",
      "Epoch [112/200], Train Loss: 0.0051\n",
      "Epoch [112/200], Test Loss: 0.0070\n",
      "Epoch [113/200], Train Loss: 0.0054\n",
      "Epoch [113/200], Test Loss: 0.0181\n",
      "Epoch [114/200], Train Loss: 0.0054\n",
      "Epoch [114/200], Test Loss: 0.0052\n",
      "Epoch [115/200], Train Loss: 0.0053\n",
      "Epoch [115/200], Test Loss: 0.0077\n",
      "Epoch [116/200], Train Loss: 0.0061\n",
      "Epoch [116/200], Test Loss: 0.0054\n",
      "Epoch [117/200], Train Loss: 0.0053\n",
      "Epoch [117/200], Test Loss: 0.0085\n",
      "Epoch [118/200], Train Loss: 0.0054\n",
      "Epoch [118/200], Test Loss: 0.0126\n",
      "Epoch [119/200], Train Loss: 0.0057\n",
      "Epoch [119/200], Test Loss: 0.0056\n",
      "Epoch [120/200], Train Loss: 0.0062\n",
      "Epoch [120/200], Test Loss: 0.0114\n",
      "Epoch [121/200], Train Loss: 0.0053\n",
      "Epoch [121/200], Test Loss: 0.0053\n",
      "Epoch [122/200], Train Loss: 0.0052\n",
      "Epoch [122/200], Test Loss: 0.0061\n",
      "Epoch [123/200], Train Loss: 0.0053\n",
      "Epoch [123/200], Test Loss: 0.0053\n",
      "Epoch [124/200], Train Loss: 0.0050\n",
      "Epoch [124/200], Test Loss: 0.0051\n",
      "Epoch [125/200], Train Loss: 0.0056\n",
      "Epoch [125/200], Test Loss: 0.0089\n",
      "Epoch [126/200], Train Loss: 0.0046\n",
      "Epoch [126/200], Test Loss: 0.0087\n",
      "Epoch [127/200], Train Loss: 0.0058\n",
      "Epoch [127/200], Test Loss: 0.0054\n",
      "Epoch [128/200], Train Loss: 0.0050\n",
      "Epoch [128/200], Test Loss: 0.0058\n",
      "Epoch [129/200], Train Loss: 0.0047\n",
      "Epoch [129/200], Test Loss: 0.0050\n",
      "Epoch [130/200], Train Loss: 0.0054\n",
      "Epoch [130/200], Test Loss: 0.0051\n",
      "Epoch [131/200], Train Loss: 0.0051\n",
      "Epoch [131/200], Test Loss: 0.0064\n",
      "Epoch [132/200], Train Loss: 0.0062\n",
      "Epoch [132/200], Test Loss: 0.0207\n",
      "Epoch [133/200], Train Loss: 0.0051\n",
      "Epoch [133/200], Test Loss: 0.0055\n",
      "Epoch [134/200], Train Loss: 0.0049\n",
      "Epoch [134/200], Test Loss: 0.0072\n",
      "Epoch [135/200], Train Loss: 0.0044\n",
      "Epoch [135/200], Test Loss: 0.0074\n",
      "Epoch [136/200], Train Loss: 0.0050\n",
      "Epoch [136/200], Test Loss: 0.0055\n",
      "Epoch [137/200], Train Loss: 0.0049\n",
      "Epoch [137/200], Test Loss: 0.0050\n",
      "Epoch [138/200], Train Loss: 0.0055\n",
      "Epoch [138/200], Test Loss: 0.0054\n",
      "Epoch [139/200], Train Loss: 0.0049\n",
      "Epoch [139/200], Test Loss: 0.0053\n",
      "Epoch [140/200], Train Loss: 0.0048\n",
      "Epoch [140/200], Test Loss: 0.0057\n",
      "Epoch [141/200], Train Loss: 0.0048\n",
      "Epoch [141/200], Test Loss: 0.0051\n",
      "Epoch [142/200], Train Loss: 0.0049\n",
      "Epoch [142/200], Test Loss: 0.0061\n",
      "Epoch [143/200], Train Loss: 0.0054\n",
      "Epoch [143/200], Test Loss: 0.0073\n",
      "Epoch [144/200], Train Loss: 0.0044\n",
      "Epoch [144/200], Test Loss: 0.0085\n",
      "Epoch [145/200], Train Loss: 0.0047\n",
      "Epoch [145/200], Test Loss: 0.0061\n",
      "Epoch [146/200], Train Loss: 0.0049\n",
      "Epoch [146/200], Test Loss: 0.0054\n",
      "Epoch [147/200], Train Loss: 0.0049\n",
      "Epoch [147/200], Test Loss: 0.0051\n",
      "Epoch [148/200], Train Loss: 0.0048\n",
      "Epoch [148/200], Test Loss: 0.0053\n",
      "Epoch [149/200], Train Loss: 0.0050\n",
      "Epoch [149/200], Test Loss: 0.0051\n",
      "Epoch [150/200], Train Loss: 0.0048\n",
      "Epoch [150/200], Test Loss: 0.0051\n",
      "Epoch [151/200], Train Loss: 0.0046\n",
      "Epoch [151/200], Test Loss: 0.0052\n",
      "Epoch [152/200], Train Loss: 0.0065\n",
      "Epoch [152/200], Test Loss: 0.0061\n",
      "Epoch [153/200], Train Loss: 0.0049\n",
      "Epoch [153/200], Test Loss: 0.0053\n",
      "Epoch [154/200], Train Loss: 0.0054\n",
      "Epoch [154/200], Test Loss: 0.0051\n",
      "Epoch [155/200], Train Loss: 0.0055\n",
      "Epoch [155/200], Test Loss: 0.0050\n",
      "Epoch [156/200], Train Loss: 0.0046\n",
      "Epoch [156/200], Test Loss: 0.0054\n",
      "Epoch [157/200], Train Loss: 0.0046\n",
      "Epoch [157/200], Test Loss: 0.0051\n",
      "Epoch [158/200], Train Loss: 0.0046\n",
      "Epoch [158/200], Test Loss: 0.0051\n",
      "Epoch [159/200], Train Loss: 0.0049\n",
      "Epoch [159/200], Test Loss: 0.0052\n",
      "Epoch [160/200], Train Loss: 0.0045\n",
      "Epoch [160/200], Test Loss: 0.0050\n",
      "Epoch [161/200], Train Loss: 0.0045\n",
      "Epoch [161/200], Test Loss: 0.0050\n",
      "Epoch [162/200], Train Loss: 0.0048\n",
      "Epoch [162/200], Test Loss: 0.0052\n",
      "Epoch [163/200], Train Loss: 0.0051\n",
      "Epoch [163/200], Test Loss: 0.0051\n",
      "Epoch [164/200], Train Loss: 0.0065\n",
      "Epoch [164/200], Test Loss: 0.0062\n",
      "Epoch [165/200], Train Loss: 0.0045\n",
      "Epoch [165/200], Test Loss: 0.0067\n",
      "Epoch [166/200], Train Loss: 0.0045\n",
      "Epoch [166/200], Test Loss: 0.0056\n",
      "Epoch [167/200], Train Loss: 0.0051\n",
      "Epoch [167/200], Test Loss: 0.0054\n",
      "Epoch [168/200], Train Loss: 0.0049\n",
      "Epoch [168/200], Test Loss: 0.0068\n",
      "Epoch [169/200], Train Loss: 0.0054\n",
      "Epoch [169/200], Test Loss: 0.0052\n",
      "Epoch [170/200], Train Loss: 0.0052\n",
      "Epoch [170/200], Test Loss: 0.0123\n",
      "Epoch [171/200], Train Loss: 0.0048\n",
      "Epoch [171/200], Test Loss: 0.0055\n",
      "Epoch [172/200], Train Loss: 0.0045\n",
      "Epoch [172/200], Test Loss: 0.0051\n",
      "Epoch [173/200], Train Loss: 0.0052\n",
      "Epoch [173/200], Test Loss: 0.0072\n",
      "Epoch [174/200], Train Loss: 0.0054\n",
      "Epoch [174/200], Test Loss: 0.0053\n",
      "Epoch [175/200], Train Loss: 0.0045\n",
      "Epoch [175/200], Test Loss: 0.0050\n",
      "Epoch [176/200], Train Loss: 0.0049\n",
      "Epoch [176/200], Test Loss: 0.0052\n",
      "Epoch [177/200], Train Loss: 0.0047\n",
      "Epoch [177/200], Test Loss: 0.0053\n",
      "Epoch [178/200], Train Loss: 0.0045\n",
      "Epoch [178/200], Test Loss: 0.0052\n",
      "Epoch [179/200], Train Loss: 0.0045\n",
      "Epoch [179/200], Test Loss: 0.0053\n",
      "Epoch [180/200], Train Loss: 0.0046\n",
      "Epoch [180/200], Test Loss: 0.0058\n",
      "Epoch [181/200], Train Loss: 0.0049\n",
      "Epoch [181/200], Test Loss: 0.0052\n",
      "Epoch [182/200], Train Loss: 0.0046\n",
      "Epoch [182/200], Test Loss: 0.0055\n",
      "Epoch [183/200], Train Loss: 0.0043\n",
      "Epoch [183/200], Test Loss: 0.0051\n",
      "Epoch [184/200], Train Loss: 0.0044\n",
      "Epoch [184/200], Test Loss: 0.0052\n",
      "Epoch [185/200], Train Loss: 0.0046\n",
      "Epoch [185/200], Test Loss: 0.0055\n",
      "Epoch [186/200], Train Loss: 0.0044\n",
      "Epoch [186/200], Test Loss: 0.0050\n",
      "Epoch [187/200], Train Loss: 0.0051\n",
      "Epoch [187/200], Test Loss: 0.0063\n",
      "Epoch [188/200], Train Loss: 0.0047\n",
      "Epoch [188/200], Test Loss: 0.0063\n",
      "Epoch [189/200], Train Loss: 0.0045\n",
      "Epoch [189/200], Test Loss: 0.0049\n",
      "Epoch [190/200], Train Loss: 0.0047\n",
      "Epoch [190/200], Test Loss: 0.0067\n",
      "Epoch [191/200], Train Loss: 0.0053\n",
      "Epoch [191/200], Test Loss: 0.0056\n",
      "Epoch [192/200], Train Loss: 0.0045\n",
      "Epoch [192/200], Test Loss: 0.0051\n",
      "Epoch [193/200], Train Loss: 0.0043\n",
      "Epoch [193/200], Test Loss: 0.0052\n",
      "Epoch [194/200], Train Loss: 0.0043\n",
      "Epoch [194/200], Test Loss: 0.0053\n",
      "Epoch [195/200], Train Loss: 0.0049\n",
      "Epoch [195/200], Test Loss: 0.0052\n",
      "Epoch [196/200], Train Loss: 0.0042\n",
      "Epoch [196/200], Test Loss: 0.0095\n",
      "Epoch [197/200], Train Loss: 0.0041\n",
      "Epoch [197/200], Test Loss: 0.0050\n",
      "Epoch [198/200], Train Loss: 0.0049\n",
      "Epoch [198/200], Test Loss: 0.0052\n",
      "Epoch [199/200], Train Loss: 0.0046\n",
      "Epoch [199/200], Test Loss: 0.0050\n",
      "Epoch [200/200], Train Loss: 0.0046\n",
      "Epoch [200/200], Test Loss: 0.0057\n"
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "\n",
    "#start training model\n",
    "for epoch in range(epochs):  # Number of epochs\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_dataloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_formula(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Train Loss: {running_loss/len(train_dataloader):.4f}\")\n",
    "\n",
    "    # Evaluate on test set\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            test_loss += loss_formula(outputs, labels).item()\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Test Loss: {test_loss/len(test_dataloader):.4f}\")\n",
    "\n",
    "    if epoch %20 == 0:\n",
    "        torch.save(model.state_dict(), \"checkpoint{}.pt\".format(epoch+1))\n",
    "torch.save(model.state_dict(), \"checkpoint{}.pt\".format(epoch+1))   #save last epoch result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
